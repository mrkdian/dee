{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from transformers import BertModel, BertTokenizer, BertConfig\n",
    "import random\n",
    "import functools\n",
    "from torchcrf import CRF\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import gc\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "from pyltp import SentenceSplitter\n",
    "from langconv import Traditional2Simplified\n",
    "import re\n",
    "import collections\n",
    "import transformer\n",
    "import copy\n",
    "import pickle\n",
    "from utils import measure_event_table_filling\n",
    "\n",
    "random.seed(666)\n",
    "torch.manual_seed(666) # cpu\n",
    "torch.cuda.manual_seed(666) #gpu\n",
    "np.random.seed(666) #numpy\n",
    "\n",
    "def strQ2B(ustring):\n",
    "    \"\"\"全角转半角\"\"\"\n",
    "    rstring = \"\"\n",
    "    for uchar in ustring:\n",
    "        inside_code=ord(uchar)\n",
    "        if inside_code == 12288:                              #全角空格直接转换            \n",
    "            inside_code = 32 \n",
    "        elif (inside_code >= 65281 and inside_code <= 65374): #全角字符（除空格）根据关系转化\n",
    "            inside_code -= 65248\n",
    "\n",
    "        rstring += chr(inside_code)\n",
    "    return rstring\n",
    "\n",
    "def sub_list_index(list, sub_list):\n",
    "    matches = []\n",
    "    for i in range(len(list)):\n",
    "        if list[i] == sub_list[0] and list[i: i+len(sub_list)] == sub_list:\n",
    "            matches.append(i)\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS_LENGTH = 500\n",
    "MAX_SENT_NUM = 5\n",
    "DEV_REAIO = 0.05\n",
    "TEXT_NORM = True\n",
    "\n",
    "# {'破产清算': {'公司名称', '公司行业', '公告时间', '受理法院', '裁定时间'},\n",
    "#  '重大安全事故': {'伤亡人数', '公司名称', '公告时间', '其他影响', '损失金额'},\n",
    "#  '股东减持': {'减持开始日期', '减持的股东', '减持金额'},\n",
    "#  '股权质押': {'接收方', '质押开始日期', '质押方', '质押结束日期', '质押金额'},\n",
    "#  '股东增持': {'增持开始日期', '增持的股东', '增持金额'},\n",
    "#  '股权冻结': {'冻结开始日期', '冻结结束日期', '冻结金额', '被冻结股东'},\n",
    "#  '高层死亡': {'公司名称', '死亡/失联时间', '死亡年龄', '高层人员', '高层职务'},\n",
    "#  '重大资产损失': {'公司名称', '公告时间', '其他损失', '损失金额'},\n",
    "#  '重大对外赔付': {'公司名称', '公告时间', '赔付对象', '赔付金额'}}\n",
    "\n",
    "EVENT_TYPES = ['破产清算', '重大安全事故', '股东减持', '股权质押', '股东增持', '股权冻结', '高层死亡', '重大资产损失', '重大对外赔付']\n",
    "\n",
    "EVENT_FIELDS = {\n",
    "    '破产清算': (['公司名称', '公司行业', '公告时间', '受理法院', '裁定时间'], ['公司', '行业', '时间', '机构', '时间']),\n",
    "    '重大安全事故': (['伤亡人数', '公司名称', '公告时间', '其他影响', '损失金额'], ['数字', '公司', '时间', '文本短语', '数字']),\n",
    "    '股东减持': (['减持开始日期', '减持的股东', '减持金额'], ['时间', '公司/人名', '数字和单位']),\n",
    "    '股权质押': (['接收方', '质押开始日期', '质押方', '质押结束日期', '质押金额'], ['公司/人名', '时间', '公司/人名', '时间', '数字']),\n",
    "    '股东增持': (['增持开始日期', '增持的股东', '增持金额'], ['时间', '公司/人名', '数字和单位']),\n",
    "    '股权冻结': (['冻结开始日期', '冻结结束日期', '冻结金额', '被冻结股东'], ['时间', '时间', '数字', '公司/人名']),\n",
    "    '高层死亡': (['公司名称', '死亡/失联时间', '死亡年龄', '高层人员', '高层职务'], ['公司', '时间', '数字', '人名', '职称']),\n",
    "    '重大资产损失': (['公司名称', '公告时间', '其他损失', '损失金额'], ['公司', '时间', '文本短语', '数字']),\n",
    "    '重大对外赔付': (['公司名称', '公告时间', '赔付对象', '赔付金额'], ['公司', '时间', '公司/人名', '数字'])\n",
    "}\n",
    "NER_LABEL_LIST = ['O']\n",
    "NER_LABEL2ID = {'O': 0}\n",
    "for ee_type, (ee_roles, ee_role_types) in EVENT_FIELDS.items():\n",
    "    for ee_role, ee_role_type in zip(ee_roles, ee_role_types):\n",
    "        if 'B-' + ee_role in NER_LABEL_LIST:\n",
    "            continue\n",
    "        NER_LABEL_LIST.append('B-' + ee_role)\n",
    "        NER_LABEL2ID[NER_LABEL_LIST[-1]] = len(NER_LABEL_LIST) - 1\n",
    "        NER_LABEL_LIST.append('I-' + ee_role)\n",
    "        NER_LABEL2ID[NER_LABEL_LIST[-1]] = len(NER_LABEL_LIST) - 1\n",
    "\n",
    "EVENT_TYPE_FIELDS_PAIRS = []\n",
    "for event_type in EVENT_TYPES:\n",
    "    fields = EVENT_FIELDS[event_type][0]\n",
    "    EVENT_TYPE_FIELDS_PAIRS.append((event_type, fields))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_config = BertConfig.from_pretrained('bert-base-chinese', cache_dir='./bert_base_chinese')\n",
    "bert_config.num_hidden_layers = 4\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese', cache_dir='./bert_base_chinese')\n",
    "bert = BertModel.from_pretrained('bert-base-chinese', cache_dir='./bert_base_chinese', config=bert_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = open('ccks 4_2 Data/event_element_dev_data.txt', mode='r', encoding='utf-8')\n",
    "train_file = open('ccks 4_2 Data/event_element_train_data_label.txt', mode='r', encoding='utf-8')\n",
    "train = []\n",
    "test = []\n",
    "for line in train_file.readlines():\n",
    "    train.append(json.loads(line))\n",
    "for line in test_file.readlines():\n",
    "    test.append(json.loads(line))\n",
    "#some label bug fix\n",
    "train[1904]['events'][0]['增持的股东'] = '微医集团（浙江）有限公司'\n",
    "train[3693]['events'][-1]['被冻结股东'] = '上海九川投资（集团）有限公司'\n",
    "train[3693]['content'] = train[3693]['content'].replace('上海九川投资(集团)有限公司', '上海九川投资（集团）有限公司')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_sum = 0\n",
    "sentence_length_sum = 0\n",
    "truncate_span = 0\n",
    "span_sum = 0\n",
    "for i, ins in enumerate(train):\n",
    "    events = ins['events']\n",
    "    content = (ins['content'])\n",
    "    if TEXT_NORM:\n",
    "        content_norm = Traditional2Simplified(strQ2B(content)).lower()\n",
    "        assert len(content) == len(content_norm)\n",
    "        \n",
    "    \n",
    "    UNK_ID = tokenizer.vocab['[UNK]']\n",
    "    PAD_ID = tokenizer.vocab['[PAD]']\n",
    "    ids = []\n",
    "    for i, char in enumerate(content):\n",
    "        if TEXT_NORM:\n",
    "            char = content_norm[i]\n",
    "        if char in tokenizer.vocab:\n",
    "            ids.append(tokenizer.vocab[char])\n",
    "        else:\n",
    "            ids.append(UNK_ID)\n",
    "    labels = [0 for _ in ids]\n",
    "    event_cls = [0 for _ in EVENT_TYPES]\n",
    "    for event in events:\n",
    "        event_cls[EVENT_TYPES.index(event['event_type'])] = 1\n",
    "        for event_role, span in event.items():\n",
    "            if event_role == 'event_type' or event_role == 'event_id' or not span:\n",
    "                continue\n",
    "            span_sum += 1\n",
    "            find_idx = -0.5\n",
    "            while find_idx != -1:\n",
    "                find_idx = content.find(span, int(find_idx + 1))\n",
    "                if find_idx != -1:\n",
    "                    assert content[find_idx: find_idx + len(span)] == span\n",
    "                    labels[find_idx] = NER_LABEL2ID['B-' + event_role]\n",
    "                    for k in range(1, len(span)):\n",
    "                        labels[find_idx + k] = NER_LABEL2ID['I-' + event_role]\n",
    "    \n",
    "    assert len(ids) == len(content) == len(labels)\n",
    "    sentences_ids = []\n",
    "    sentences_labels = []\n",
    "    \n",
    "    sentences = []\n",
    "    raw_sentences = list(filter(lambda x: bool(x), re.split('([^。；]+[。；])', content)))\n",
    "    curr_pos = 0\n",
    "    sentence_sum += len(raw_sentences)\n",
    "    for sentence in raw_sentences:\n",
    "        sentence_length_sum += len(sentence)\n",
    "        # print(len(sentence))\n",
    "        if len(sentence) < MAX_TOKENS_LENGTH:\n",
    "            sentences.append(sentence)\n",
    "            curr_pos += len(sentence)\n",
    "        else:\n",
    "            while len(sentence) > 0:\n",
    "                sentences.append(sentence[:MAX_TOKENS_LENGTH])\n",
    "                curr_pos += len(sentences[-1])\n",
    "                if curr_pos < len(labels) and labels[curr_pos] != 0 and labels[curr_pos -1] != 0:\n",
    "                    truncate_span += 1\n",
    "                    #print(truncate_span / span_sum)\n",
    "                sentence = sentence[MAX_TOKENS_LENGTH:]\n",
    "    \n",
    "    merge_sentences = []\n",
    "    curr_sentence = ''\n",
    "    for sentence in sentences:\n",
    "        if len(sentence) + len(curr_sentence) <= MAX_TOKENS_LENGTH:\n",
    "            curr_sentence += sentence\n",
    "        else:\n",
    "            merge_sentences.append(curr_sentence)\n",
    "            curr_sentence = sentence\n",
    "    if curr_sentence:\n",
    "        merge_sentences.append(curr_sentence)\n",
    "    \n",
    "    curr_pos = 0\n",
    "    ids_list = []\n",
    "    labels_list = []\n",
    "    attention_mask = []\n",
    "    ids_length = []\n",
    "    if len(merge_sentences) > 3:\n",
    "        filted_merge_sentences = []\n",
    "        for sentence in merge_sentences:\n",
    "            if sum(labels[curr_pos: curr_pos + len(sentence)]) == 0:\n",
    "                curr_pos += len(sentence)\n",
    "            else:\n",
    "                filted_merge_sentences.append(sentence)\n",
    "                ids_list.append(ids[curr_pos: curr_pos + len(sentence)])\n",
    "                labels_list.append(labels[curr_pos: curr_pos + len(sentence)])\n",
    "                attention_mask.append([1 for _ in range(len(sentence))])\n",
    "                ids_length.append(len(ids_list[-1]))\n",
    "                if len(ids_list[-1]) < MAX_TOKENS_LENGTH:\n",
    "                    pad_num = MAX_TOKENS_LENGTH - len(ids_list[-1])\n",
    "                    ids_list[-1].extend([0 for _ in range(pad_num)])\n",
    "                    labels_list[-1].extend([-1 for _ in range(pad_num)])\n",
    "                    attention_mask[-1].extend([0 for _ in range(pad_num)])\n",
    "                curr_pos += len(sentence)\n",
    "                assert ids_length[-1] == sum(attention_mask[-1])\n",
    "        \n",
    "        assert len(filted_merge_sentences) == len(ids_list) == len(labels_list) == len(attention_mask)\n",
    "        if len(filted_merge_sentences) > MAX_SENT_NUM: # truncate sentence\n",
    "            filted_merge_sentences = filted_merge_sentences[:MAX_SENT_NUM]\n",
    "            ids_list = ids_list[:MAX_SENT_NUM]\n",
    "            labels_list = labels_list[:MAX_SENT_NUM]\n",
    "            attention_mask = attention_mask[:MAX_SENT_NUM]\n",
    "            ids_length = ids_length[:MAX_SENT_NUM]\n",
    "        ins['merge_sentences'] = filted_merge_sentences\n",
    "        assert len(filted_merge_sentences) == len(ids_list) == len(labels_list) == len(attention_mask)\n",
    "    else:\n",
    "        for sentence in merge_sentences:\n",
    "            ids_list.append(ids[curr_pos: curr_pos + len(sentence)])\n",
    "            labels_list.append(labels[curr_pos: curr_pos + len(sentence)])\n",
    "            attention_mask.append([1 for _ in range(len(sentence))])\n",
    "            ids_length.append(len(ids_list[-1]))\n",
    "            if len(ids_list[-1]) < MAX_TOKENS_LENGTH:\n",
    "                pad_num = MAX_TOKENS_LENGTH - len(ids_list[-1])\n",
    "                ids_list[-1].extend([0 for _ in range(pad_num)])\n",
    "                labels_list[-1].extend([-1 for _ in range(pad_num)])\n",
    "                attention_mask[-1].extend([0 for _ in range(pad_num)])\n",
    "            curr_pos += len(sentence)\n",
    "            assert ids_length[-1] == sum(attention_mask[-1])\n",
    "        ins['merge_sentences'] = merge_sentences\n",
    "        assert len(merge_sentences) == len(ids_list) == len(labels_list) == len(attention_mask)\n",
    "    ins['ids_list'] = ids_list\n",
    "    ins['labels_list'] = labels_list\n",
    "    ins['attention_mask'] = attention_mask\n",
    "    ins['ids'] = ids\n",
    "    ins['labels'] = labels\n",
    "    ins['event_cls'] = event_cls\n",
    "    ins['ids_length'] = ids_length\n",
    "    assert ''.join(merge_sentences) == content\n",
    "#doc_id: 2047486 for test\n",
    "# chr(1627)\n",
    "random.shuffle(train)\n",
    "dev_num = math.ceil(len(train) * DEV_REAIO)\n",
    "dev = train[:dev_num]\n",
    "part_train = train[dev_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_label(labels_list, attention_mask, ids_list):\n",
    "    span_token_drange_list = []\n",
    "    for sent_idx, labels in enumerate(labels_list):\n",
    "        mask = attention_mask[sent_idx]\n",
    "        ids = ids_list[sent_idx]\n",
    "        seq_len = len(labels)\n",
    "        char_s = 0\n",
    "        while char_s < seq_len:\n",
    "            if mask[char_s] == 0:\n",
    "                break\n",
    "            entity_idx = labels[char_s]\n",
    "            if entity_idx % 2 == 1:\n",
    "                char_e = char_s + 1\n",
    "                while char_e < seq_len and mask[char_e] == 1 and labels[char_e] == entity_idx + 1:\n",
    "                    char_e += 1\n",
    "\n",
    "                token_tup = tuple(ids[char_s:char_e])\n",
    "                drange = (sent_idx, char_s, char_e)\n",
    "\n",
    "                span_token_drange_list.append((token_tup, drange, entity_idx))\n",
    "\n",
    "                char_s = char_e\n",
    "            else:\n",
    "                char_s += 1\n",
    "    span_token_drange_list.sort(key=lambda x: x[-1])  # sorted by drange = (sent_idx, char_s, char_e)\n",
    "    token_tup2dranges = collections.OrderedDict()\n",
    "    for token_tup, drange, entity_idx in span_token_drange_list:\n",
    "        # print(tokenizer.decode(token_tup), NER_LABEL_LIST[entity_idx])\n",
    "        if token_tup not in token_tup2dranges:\n",
    "            token_tup2dranges[token_tup] = []\n",
    "        token_tup2dranges[token_tup].append((drange, entity_idx))\n",
    "    return token_tup2dranges\n",
    "\n",
    "# for i in range(10):\n",
    "#     ret = collate_label(train[i]['labels_list'], train[i]['attention_mask'], train[i]['ids_list'])\n",
    "#     print('------------')\n",
    "\n",
    "def measure_dee_prediction(doc_decode_res, batch_dev, EVENT_TYPES, EVENT_FIELDS, EVENT_TYPE_FIELDS_PAIRS):\n",
    "    for i, decode_res in enumerate(doc_decode_res):\n",
    "        merge_sentences = batch_dev[i]['merge_sentences']\n",
    "        for event_idx, res in enumerate(decode_res):\n",
    "            if res is None:\n",
    "                continue\n",
    "            for fields in res:\n",
    "                for j, drange in enumerate(fields):\n",
    "                    if drange is None:\n",
    "                        continue\n",
    "                    sent_idx, char_s, char_e = drange\n",
    "                    fields[j] = merge_sentences[sent_idx][char_s: char_e]\n",
    "    \n",
    "    gt_decode_res = []\n",
    "    for ins in batch_dev:\n",
    "        decode_res = [None for _ in EVENT_TYPES]\n",
    "        events = ins['events']\n",
    "        for event in events:\n",
    "            event_type = event['event_type']\n",
    "            event_idx = EVENT_TYPES.index(event_type)\n",
    "            if decode_res[event_idx] == None:\n",
    "                decode_res[event_idx] = []\n",
    "            event_fields = EVENT_FIELDS[event_type]\n",
    "            res = []\n",
    "            for field in event_fields[0]:\n",
    "                span = None\n",
    "                if field in event and event[field]:\n",
    "                    span = event[field]\n",
    "                res.append(span)\n",
    "            decode_res[event_idx].append(res)\n",
    "        gt_decode_res.append(decode_res)\n",
    "    eval_res = measure_event_table_filling(doc_decode_res, gt_decode_res, EVENT_TYPE_FIELDS_PAIRS, dict_return=True)\n",
    "    return eval_res\n",
    "#gg = measure_dee_prediction(copy.deepcopy(res), batch_dev, EVENT_TYPES, EVENT_FIELDS, EVENT_TYPE_FIELDS_PAIRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_drange2text(doc_decode_res, batch_test):\n",
    "    for i, decode_res in enumerate(doc_decode_res):\n",
    "        merge_sentences = batch_test[i]['merge_sentences']\n",
    "        for event_idx, res in enumerate(decode_res):\n",
    "            if res is None:\n",
    "                continue\n",
    "            for fields in res:\n",
    "                for j, drange in enumerate(fields):\n",
    "                    if drange is None:\n",
    "                        continue\n",
    "                    sent_idx, char_s, char_e = drange\n",
    "                    fields[j] = merge_sentences[sent_idx][char_s: char_e]\n",
    "\n",
    "def preprocess_test(test, tokenizer):\n",
    "    for i, ins in enumerate(test):\n",
    "        print(list(ins.keys()))\n",
    "        content = ins['content']\n",
    "        UNK_ID = tokenizer.vocab['[UNK]']\n",
    "        PAD_ID = tokenizer.vocab['[PAD]']\n",
    "        ids = []\n",
    "        for char in content:\n",
    "            if char in tokenizer.vocab:\n",
    "                ids.append(tokenizer.vocab[char])\n",
    "            else:\n",
    "                ids.append(UNK_ID)\n",
    "\n",
    "        sentences_ids = []\n",
    "        sentences = []\n",
    "        raw_sentences = list(filter(lambda x: bool(x), re.split('([^。；]+[。；])', content)))\n",
    "        curr_pos = 0\n",
    "        for sentence in raw_sentences:\n",
    "            if len(sentence) < MAX_TOKENS_LENGTH:\n",
    "                sentences.append(sentence)\n",
    "                curr_pos += len(sentence)\n",
    "            else:\n",
    "                while len(sentence) > 0:\n",
    "                    sentences.append(sentence[:MAX_TOKENS_LENGTH])\n",
    "                    curr_pos += len(sentences[-1])\n",
    "                    sentence = sentence[MAX_TOKENS_LENGTH:]\n",
    "        \n",
    "        merge_sentences = []\n",
    "        curr_sentence = ''\n",
    "        for sentence in sentences:\n",
    "            if len(sentence) + len(curr_sentence) <= MAX_TOKENS_LENGTH:\n",
    "                curr_sentence += sentence\n",
    "            else:\n",
    "                merge_sentences.append(curr_sentence)\n",
    "                curr_sentence = sentence\n",
    "        if curr_sentence:\n",
    "            merge_sentences.append(curr_sentence)\n",
    "\n",
    "        curr_pos = 0\n",
    "        ids_list = []\n",
    "        attention_mask = []\n",
    "        ids_length = []\n",
    "        print(len(merge_sentences))\n",
    "        if len(merge_sentences) > 3:\n",
    "            filted_merge_sentences = []\n",
    "            for sentence in merge_sentences:\n",
    "                if sum(labels[curr_pos: curr_pos + len(sentence)]) == 0:\n",
    "                    curr_pos += len(sentence)\n",
    "                else:\n",
    "                    filted_merge_sentences.append(sentence)\n",
    "                    ids_list.append(ids[curr_pos: curr_pos + len(sentence)])\n",
    "                    attention_mask.append([1 for _ in range(len(sentence))])\n",
    "                    ids_length.append(len(ids_list[-1]))\n",
    "                    if len(ids_list[-1]) < MAX_TOKENS_LENGTH:\n",
    "                        pad_num = MAX_TOKENS_LENGTH - len(ids_list[-1])\n",
    "                        ids_list[-1].extend([0 for _ in range(pad_num)])\n",
    "                        attention_mask[-1].extend([0 for _ in range(pad_num)])\n",
    "                    curr_pos += len(sentence)\n",
    "                    assert ids_length[-1] == sum(attention_mask[-1])\n",
    "\n",
    "            assert len(filted_merge_sentences) == len(ids_list) == len(attention_mask)\n",
    "            if len(filted_merge_sentences) > MAX_SENT_NUM: # truncate sentence\n",
    "                filted_merge_sentences = filted_merge_sentences[:MAX_SENT_NUM]\n",
    "                ids_list = ids_list[:MAX_SENT_NUM]\n",
    "                attention_mask = attention_mask[:MAX_SENT_NUM]\n",
    "                ids_length = ids_length[:MAX_SENT_NUM]\n",
    "            ins['merge_sentences'] = filted_merge_sentences\n",
    "            assert len(filted_merge_sentences) == len(ids_list) == len(attention_mask)\n",
    "        else:\n",
    "            for sentence in merge_sentences:\n",
    "                ids_list.append(ids[curr_pos: curr_pos + len(sentence)])\n",
    "                attention_mask.append([1 for _ in range(len(sentence))])\n",
    "                ids_length.append(len(ids_list[-1]))\n",
    "                if len(ids_list[-1]) < MAX_TOKENS_LENGTH:\n",
    "                    pad_num = MAX_TOKENS_LENGTH - len(ids_list[-1])\n",
    "                    ids_list[-1].extend([0 for _ in range(pad_num)])\n",
    "                    attention_mask[-1].extend([0 for _ in range(pad_num)])\n",
    "                curr_pos += len(sentence)\n",
    "                assert ids_length[-1] == sum(attention_mask[-1])\n",
    "            ins['merge_sentences'] = merge_sentences\n",
    "            assert len(merge_sentences) == len(ids_list) == len(attention_mask)\n",
    "        ins['ids_list'] = ids_list\n",
    "        ins['attention_mask'] = attention_mask\n",
    "        ins['ids'] = ids\n",
    "        ins['ids_length'] = ids_length\n",
    "        assert ''.join(merge_sentences) == content  \n",
    "\n",
    "def eval_save_test(origin_test, tokenizer, save_file_name):\n",
    "    TEST_DOC_BATCH_SIZE = 2\n",
    "    TEST_BATCH_NUM = math.ceil(len(origin_test) / TEST_DOC_BATCH_SIZE)\n",
    "    TEST_SAVE_DIR = '%s/test_res' % OUTPUT_DIR\n",
    "    if not os.path.exists(TEST_SAVE_DIR):\n",
    "        os.mkdir(TEST_SAVE_DIR)\n",
    "    save_file_name = os.path.join(TEST_SAVE_DIR, save_file_name)\n",
    "    \n",
    "    test = copy.deepcopy(origin_test)\n",
    "    preprocess_test(test, tokenizer)\n",
    "    model.eval()\n",
    "    total_decode_res = []\n",
    "    with tqdm(total=TEST_BATCH_NUM) as pbar:\n",
    "        for batch_num in range(TEST_BATCH_NUM):\n",
    "            batch_beg = batch_num * TEST_DOC_BATCH_SIZE\n",
    "            batch_end = (batch_num + 1) * TEST_DOC_BATCH_SIZE\n",
    "            batch_test = test[batch_beg: batch_end]\n",
    "\n",
    "            _, doc_decode_res = model(batch_test, train_flag=False, use_gold=False)\n",
    "            total_decode_res.extend(doc_decode_res)\n",
    "            pbar.update()\n",
    "    decode_drange2text(total_decode_res, test)\n",
    "    test_copy = copy.deepcopy(origin_test)\n",
    "    for ins, decode_res, ins_copy in zip(test, total_decode_res, test_copy):\n",
    "        mult_ans = []\n",
    "        for event_idx, mult_res in enumerate(decode_res):\n",
    "            if mult_res is None or len(mult_res) < 1:\n",
    "                continue\n",
    "            event_type = EVENT_TYPES[event_idx]\n",
    "            event_fields = EVENT_FIELDS[event_type][0]\n",
    "            for res in mult_res:\n",
    "                ans = {'event_type': event_type}\n",
    "                for field_idx, span in enumerate(res):\n",
    "                    if span is None:\n",
    "                        continue\n",
    "                    ans[event_fields[field_idx]] = span\n",
    "                mult_ans.append(ans)\n",
    "        ins_copy['events'] = mult_ans\n",
    "    # json.save(test_copy, open(save_file_name, mode='w', encoding='utf-8'))\n",
    "    with_content = open(save_file_name + '-with_content', mode='w', encoding='utf-8')\n",
    "    with open(save_file_name, mode='w', encoding='utf-8') as f:\n",
    "        for ins in test_copy:\n",
    "            write_obj = {}\n",
    "            write_obj['doc_id'] = ins['doc_id']\n",
    "            write_obj['events'] = ins['events']\n",
    "            f.write(json.dumps(write_obj, ensure_ascii=False) + '\\n')\n",
    "            with_content.write(json.dumps(ins, ensure_ascii=False) + '\\n')\n",
    "    return test_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventTable(nn.Module):\n",
    "    def __init__(self, event_type, field_types, hidden_size):\n",
    "        super(EventTable, self).__init__()\n",
    "\n",
    "        self.event_type = event_type\n",
    "        self.field_types = field_types\n",
    "        self.num_fields = len(field_types)\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.event_cls = nn.Linear(hidden_size, 2)  # 0: NA, 1: trigger this event\n",
    "        self.field_cls_list = nn.ModuleList(\n",
    "            # 0: NA, 1: trigger this field\n",
    "            [nn.Linear(hidden_size, 2) for _ in range(self.num_fields)]\n",
    "        )\n",
    "\n",
    "        # used to aggregate sentence and span embedding\n",
    "        self.event_query = nn.Parameter(torch.Tensor(1, self.hidden_size))\n",
    "        # used for fields that do not contain any valid span\n",
    "        # self.none_span_emb = nn.Parameter(torch.Tensor(1, self.hidden_size))\n",
    "        # used for aggregating history filled span info\n",
    "        self.field_queries = nn.ParameterList(\n",
    "            [nn.Parameter(torch.Tensor(1, self.hidden_size)) for _ in range(self.num_fields)]\n",
    "        )\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.hidden_size)\n",
    "        self.event_query.data.uniform_(-stdv, stdv)\n",
    "        # self.none_span_emb.data.uniform_(-stdv, stdv)\n",
    "        for fq in self.field_queries:\n",
    "            fq.data.uniform_(-stdv, stdv)\n",
    "    \n",
    "    def forward(self, sent_context_emb=None, batch_span_emb=None, field_idx=None):\n",
    "        assert (sent_context_emb is None) ^ (batch_span_emb is None)\n",
    "\n",
    "        if sent_context_emb is not None:  # [num_spans+num_sents, hidden_size]\n",
    "            # doc_emb.size = [1, hidden_size]\n",
    "            doc_emb, _ = transformer.attention(self.event_query, sent_context_emb, sent_context_emb)\n",
    "            doc_pred_logits = self.event_cls(doc_emb)\n",
    "            doc_pred_logp = F.log_softmax(doc_pred_logits, dim=-1)\n",
    "\n",
    "            return doc_pred_logp\n",
    "\n",
    "        if batch_span_emb is not None:\n",
    "            assert field_idx is not None\n",
    "            # span_context_emb: [batch_size, hidden_size] or [hidden_size]\n",
    "            if batch_span_emb.dim() == 1:\n",
    "                batch_span_emb = batch_span_emb.unsqueeze(0)\n",
    "            span_pred_logits = self.field_cls_list[field_idx](batch_span_emb)\n",
    "            span_pred_logp = F.log_softmax(span_pred_logits, dim=-1)\n",
    "\n",
    "            return span_pred_logp\n",
    "        \n",
    "class SentencePosEncoder(nn.Module):\n",
    "    def __init__(self, hidden_size, max_sent_num=100, dropout=0.1):\n",
    "        super(SentencePosEncoder, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(max_sent_num, hidden_size)\n",
    "        self.layer_norm = transformer.LayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, batch_elem_emb, sent_pos_ids=None):\n",
    "        if sent_pos_ids is None:\n",
    "            num_elem = batch_elem_emb.size(-2)\n",
    "            sent_pos_ids = torch.arange(\n",
    "                num_elem, dtype=torch.long, device=batch_elem_emb.device, requires_grad=False\n",
    "            )\n",
    "        elif not isinstance(sent_pos_ids, torch.Tensor):\n",
    "            sent_pos_ids = torch.tensor(\n",
    "                sent_pos_ids, dtype=torch.long, device=batch_elem_emb.device, requires_grad=False\n",
    "            )\n",
    "\n",
    "        batch_pos_emb = self.embedding(sent_pos_ids)\n",
    "        out = batch_elem_emb + batch_pos_emb\n",
    "        out = self.dropout(self.layer_norm(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fin\n"
     ]
    }
   ],
   "source": [
    "class DocEE(nn.Module):\n",
    "    def __init__(self, config, basic_encoder):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.init_eval_obj()\n",
    "        \n",
    "        self.basic_encoder = basic_encoder\n",
    "        if config['use_crf']:\n",
    "            self.crf = CRF(num_tags=len(NER_LABEL_LIST), batch_first=True)\n",
    "        self.seq_labeler = nn.Sequential(\n",
    "            nn.Linear(self.basic_encoder.config.hidden_size, self.basic_encoder.config.hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.basic_encoder.config.hidden_size // 2, len(NER_LABEL_LIST))\n",
    "        )\n",
    "        self.event_tables = nn.ModuleList([\n",
    "            EventTable(event_type, self.config['EVENT_FIELDS'][event_type], self.basic_encoder.config.hidden_size)\n",
    "            for event_type in self.config['EVENT_TYPES']\n",
    "        ])\n",
    "        self.sent_pos_encoder = SentencePosEncoder(\n",
    "            basic_encoder.config.hidden_size, max_sent_num=config['MAX_SENT_NUM'], dropout=basic_encoder.config.hidden_dropout_prob\n",
    "        )\n",
    "    def init_eval_obj(self):\n",
    "        self.eval_obj = {'event_type_pred': [], 'event_type_gt': [], 'ner_pred': [], 'ner_gt': [] }\n",
    "    \n",
    "    def forward(self, batch_train, train_flag=True, use_gold=True, ee_method='GreedyDec'):\n",
    "        ner_loss, doc_ids_emb, doc_ner_pred, doc_sent_emb = self.do_ner(batch_train, train_flag, use_gold)\n",
    "        if ee_method == 'GreedyDec':\n",
    "            decode_loss, decode_res = self.greedy_dec(doc_sent_emb, doc_ner_pred, batch_train, train_flag)\n",
    "            total_loss = ner_loss + decode_loss\n",
    "            return total_loss, decode_res\n",
    "        else:\n",
    "            raise Exception('Not support event method: ' + ee_method)\n",
    "    \n",
    "    def pooling(self, emb, emb_length, attention_mask, sent_pos_ids):\n",
    "        # emb.shape should be (batch_size, sent_length, hidden_size)\n",
    "        pooling_type = self.config['POOLING']\n",
    "        if pooling_type == 'max':\n",
    "            attention_mask = attention_mask.unsqueeze(dim=-1)\n",
    "            emb.masked_fill(attention_mask == 0, -float('inf'))\n",
    "            pooling_emb = torch.max(emb, dim=1)[0]\n",
    "        else:\n",
    "            raise Exception('Not support pooling method: ' + pooling_type)\n",
    "        \n",
    "        pooling_emb = self.sent_pos_encoder(pooling_emb, sent_pos_ids)\n",
    "        return pooling_emb\n",
    "    \n",
    "    def greedy_dec(self, doc_sent_emb, doc_ner_pred, batch_train, train_flag=True):\n",
    "        doc_decode_res = []\n",
    "        event_cls_loss = 0\n",
    "        for sent_emb, ner_pred, ins in zip(doc_sent_emb, doc_ner_pred, batch_train):\n",
    "            event_type_score = []\n",
    "            for event_table in self.event_tables:\n",
    "                 event_type_score.append(event_table(sent_context_emb=sent_emb))\n",
    "            event_type_score = torch.cat(event_type_score, dim=0)\n",
    "            \n",
    "            if train_flag:\n",
    "                event_type_label = torch.tensor(ins['event_cls'], device=event_type_score.device)\n",
    "                event_cls_loss += F.nll_loss(event_type_score, event_type_label)\n",
    "            else:\n",
    "                span2drange = collate_label(ner_pred, ins['attention_mask'], ins['ids_list'])\n",
    "                event_type_pred = torch.argmax(event_type_score, dim=-1).tolist()\n",
    "                self.eval_obj['event_type_pred'].append(event_type_pred)\n",
    "                if ins.get('event_cls') is not None:\n",
    "                    self.eval_obj['event_type_gt'].append(ins['event_cls'])\n",
    "                # event_type_pred = ins['event_cls']\n",
    "                \n",
    "                EVENT_TYPES = self.config['EVENT_TYPES']\n",
    "                EVENT_FIELDS = self.config['EVENT_FIELDS']\n",
    "                NER_LABEL_LIST = self.config['NER_LABEL_LIST']\n",
    "                label2drange = {}\n",
    "                for span, dranges in span2drange.items():\n",
    "                    for drange, label_idx in dranges:\n",
    "                        label = NER_LABEL_LIST[label_idx]\n",
    "                        #assert label.startswith('B-')\n",
    "                        label = label[2:]\n",
    "                        if label not in label2drange:\n",
    "                            label2drange[label] = []\n",
    "                        label2drange[label].append(drange)\n",
    "                decode_res = []\n",
    "                for event_idx, pred in enumerate(event_type_pred):\n",
    "                    if pred == 0:\n",
    "                        decode_res.append(None)\n",
    "                        continue\n",
    "                    event_type = EVENT_TYPES[event_idx]\n",
    "                    fields = EVENT_FIELDS[event_type][0]\n",
    "                    field_res = []\n",
    "                    for field in fields:\n",
    "                        if field not in label2drange:\n",
    "                            field_res.append(None)\n",
    "                            continue\n",
    "                        field_res.append(label2drange[field][0]) # greedy\n",
    "                    decode_res.append([field_res])\n",
    "                doc_decode_res.append(decode_res)\n",
    "        event_cls_loss /= len(doc_sent_emb) #mean for batch\n",
    "        return event_cls_loss, doc_decode_res\n",
    "        \n",
    "    \n",
    "    def do_ner(self, batch_train, train_flag=True, use_gold=False):\n",
    "        device = self.basic_encoder.device\n",
    "        input_ids = []\n",
    "        ner_label = []\n",
    "        attention_mask = []\n",
    "        ids_length = []\n",
    "        sent_pos_ids = []\n",
    "        doc_beg_list = [0]\n",
    "        for ins in batch_train:\n",
    "            if train_flag:\n",
    "                ner_label.extend(ins['labels_list'])\n",
    "            input_ids.extend(ins['ids_list'])\n",
    "            attention_mask.extend(ins['attention_mask'])\n",
    "            ids_length.extend(ins['ids_length'])\n",
    "            sent_pos_ids.extend(list(range(len(ins['ids_list']))))\n",
    "            doc_beg_list.append(len(input_ids))\n",
    "        \n",
    "        input_ids = torch.tensor(input_ids, device=device, dtype=torch.long)\n",
    "        attention_mask = torch.tensor(attention_mask, device=device, dtype=torch.float)\n",
    "        batch_emb = self.basic_encoder(input_ids, attention_mask=attention_mask)[0]\n",
    "        ner_score = self.seq_labeler(batch_emb)\n",
    "        pooling_emb = self.pooling(batch_emb, ids_length, attention_mask, sent_pos_ids)\n",
    "        ner_loss = 0\n",
    "        if train_flag:\n",
    "            self.eval_obj['ner_gt'].append(ner_label)\n",
    "            ner_label = torch.tensor(ner_label, device=device, dtype=torch.long)\n",
    "            if self.config['use_crf']:\n",
    "                attention_mask = attention_mask.to(torch.uint8)\n",
    "                ner_loss = -self.crf(ner_score, ner_label, mask=attention_mask, reduction='mean')\n",
    "            else:\n",
    "                ner_loss = F.cross_entropy(ner_score.view(-1, len(self.config['NER_LABEL_LIST'])), ner_label.view(-1), ignore_index=-1)\n",
    "        if use_gold:\n",
    "            ner_pred = ner_label\n",
    "        else:\n",
    "            if self.config['use_crf']:\n",
    "                ner_pred = self.crf.decode(ner_score, mask=attention_mask.to(dtype=torch.uint8))\n",
    "            else:\n",
    "                ner_pred = torch.argmax(ner_score, dim=-1).tolist()\n",
    "        self.eval_obj['ner_pred'].append(ner_pred)\n",
    "        doc_ids_emb = []\n",
    "        doc_ner_pred = []\n",
    "        doc_sent_emb = []\n",
    "        for i in range(len(doc_beg_list) - 1):\n",
    "            doc_beg, doc_end = doc_beg_list[i], doc_beg_list[i + 1]\n",
    "            doc_ids_emb.append(batch_emb[doc_beg: doc_end])\n",
    "            doc_ner_pred.append(ner_pred[doc_beg: doc_end])\n",
    "            doc_sent_emb.append(pooling_emb[doc_beg: doc_end])\n",
    "        return ner_loss, doc_ids_emb, doc_ner_pred, doc_sent_emb\n",
    "            \n",
    "config = {\n",
    "    'use_crf': True,\n",
    "    'SENT_BATCH_SIZE': 4,\n",
    "    'POOLING': 'max',\n",
    "    'EVENT_FIELDS': EVENT_FIELDS,\n",
    "    'EVENT_TYPES': EVENT_TYPES,\n",
    "    'NER_LABEL_LIST': NER_LABEL_LIST,\n",
    "    'NER_LABEL2ID': NER_LABEL2ID,\n",
    "    'MAX_SENT_NUM': MAX_SENT_NUM\n",
    "}\n",
    "model = DocEE(config, bert)\n",
    "model.cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "print('fin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/99 [00:00<00:11,  8.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:12<00:00,  7.86it/s]\n",
      " 18%|█▊        | 65/370 [00:07<00:34,  8.81it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-42ddf86c7293>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEVAL_JSON_FILE\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEVAL_OBJ_FILE\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0meval_save_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTEST_FILE\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_json\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'MicroF1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-157741089cef>\u001b[0m in \u001b[0;36meval_save_test\u001b[0;34m(origin_test, tokenizer, save_file_name)\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0mbatch_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_beg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_end\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_decode_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_flag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0mtotal_decode_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_decode_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-6b89f8af51b8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batch_train, train_flag, use_gold, ee_method)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_flag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mee_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'GreedyDec'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mner_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_ids_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_ner_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_sent_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_ner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_flag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mee_method\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'GreedyDec'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mdecode_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgreedy_dec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_sent_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_ner_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_flag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-6b89f8af51b8>\u001b[0m in \u001b[0;36mdo_ner\u001b[0;34m(self, batch_train, train_flag, use_gold)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'use_crf'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m                 \u001b[0mner_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mner_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0mner_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mner_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python36/lib/python3.6/site-packages/torchcrf/__init__.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, emissions, mask)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_viterbi_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memissions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     def _validate(\n",
      "\u001b[0;32m~/anaconda3/envs/python36/lib/python3.6/site-packages/torchcrf/__init__.py\u001b[0m in \u001b[0;36m_viterbi_decode\u001b[0;34m(self, emissions, mask)\u001b[0m\n\u001b[1;32m    303\u001b[0m             \u001b[0;31m# and save the index that produces the next score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;31m# shape: (batch_size, num_tags)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m             \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCH = 30\n",
    "DOC_BATCH_SIZE = 2\n",
    "EVAL_DOC_BATCH_SIZE = 2\n",
    "\n",
    "# dev = part_train\n",
    "BATCH_NUM = math.ceil(len(part_train) / DOC_BATCH_SIZE)\n",
    "EVAL_BATCH_NUM = math.ceil(len(dev) / EVAL_DOC_BATCH_SIZE)\n",
    "\n",
    "OUTPUT_DIR = 'output'\n",
    "MODEL_SAVE_DIR = '%s/save_model' % OUTPUT_DIR\n",
    "EVAL_SAVE_DIR = '%s/save_eval' % OUTPUT_DIR\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.mkdir(OUTPUT_DIR)\n",
    "if not os.path.exists(MODEL_SAVE_DIR):\n",
    "    os.mkdir(MODEL_SAVE_DIR)\n",
    "if not os.path.exists(EVAL_SAVE_DIR):\n",
    "    os.mkdir(EVAL_SAVE_DIR)\n",
    "\n",
    "EVAL_JSON_FILE = os.path.join(EVAL_SAVE_DIR, 'eval-%d.json')\n",
    "EVAL_OBJ_FILE = os.path.join(EVAL_SAVE_DIR, 'eval-obj-%d.pkl')\n",
    "TEST_FILE = 'test-%d.txt'\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    print('%d-----------------' % epoch)\n",
    "#     print_loss = 0\n",
    "#     random.shuffle(part_train)\n",
    "#     model.train()\n",
    "#     with tqdm(total=BATCH_NUM) as pbar:\n",
    "#         for batch_num in range(BATCH_NUM):\n",
    "#             batch_beg = batch_num * DOC_BATCH_SIZE\n",
    "#             batch_end = (batch_num + 1) * DOC_BATCH_SIZE\n",
    "#             batch_train = part_train[batch_beg: batch_end]\n",
    "            \n",
    "#             #optimizer.zero_grad()\n",
    "#             loss, _ = model(batch_train, train_flag=True, use_gold=True)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             model.zero_grad()\n",
    "#             print_loss += loss.cpu().detach().numpy()\n",
    "#             pbar.set_description('total_loss: %f' % (print_loss / (batch_num + 1)))\n",
    "#             pbar.update()\n",
    "    model.eval()\n",
    "    model.init_eval_obj()\n",
    "    total_decode_res = []\n",
    "    with tqdm(total=EVAL_BATCH_NUM) as pbar:\n",
    "        for batch_num in range(EVAL_BATCH_NUM):\n",
    "            batch_beg = batch_num * EVAL_DOC_BATCH_SIZE\n",
    "            batch_end = (batch_num + 1) * EVAL_DOC_BATCH_SIZE\n",
    "            batch_dev = dev[batch_beg: batch_end]\n",
    "\n",
    "            _, doc_decode_res = model(batch_dev, train_flag=False, use_gold=False)\n",
    "            total_decode_res.extend(doc_decode_res)\n",
    "            pbar.update()\n",
    "    eval_json = measure_dee_prediction(total_decode_res, dev, EVENT_TYPES, EVENT_FIELDS, EVENT_TYPE_FIELDS_PAIRS)\n",
    "    json.dump(eval_json, open(EVAL_JSON_FILE % epoch, mode='w', encoding='utf-8'))\n",
    "    pickle.dump(model.eval_obj, open(EVAL_OBJ_FILE % epoch, mode='wb'))\n",
    "    eval_save_test(test, tokenizer, TEST_FILE % epoch)\n",
    "    print(eval_json[-1]['MicroF1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2892"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.session.delete();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
